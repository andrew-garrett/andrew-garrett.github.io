<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="shortcut icon" type="image/png" href="../../assets/images/FAVICON.png" />

    <!-- Put your site title here -->
    <title>
        OAK-Deep
    </title>

    <meta name="description" content="Project OAK-Deep">
    <!-- Add some coding keywords below, Ex: (React, CSS etc) -->
    <meta name="keywords" content="Andrew Garrett, Robotics, Computer Vision, Deep Learning, Machine Learning" />
    <link rel="stylesheet" href="../../index.css" />
</head>

<body>

  <!-- ***** Header ***** -->

    <header class="header" role="banner" id="top">
        <div class="row">
            <nav class="nav" role="navigation">
                <ul class="nav__items">
                  <!-- <li class="nav__item">
                    <a href="../../index.html" class="nav__link">home</a>
                  </li> -->
                </ul>
            </nav>
        </div>
        <div class="header__text-box row">
            <div class="header__text">
                <h1 class="heading-primary">
                    <!-- Project title -->
                    <span>
                        Project OAK-Deep
                        <a target="_blank" rel="noopener noreferrer" href="https://github.com/andrew-garrett/oakd-research" title="View Source Code" style="float: right;">
                            <img src="../../assets/icons/github.svg" height="50px" alt="GitHub">
                        </a>
                    </span>
                </h1>
                <!-- Put a small paragraph about the project -->
                <p>An experimental toolbox for deep computer vision on the OAK-D Camera</p>
                <a href="../../index.html#projects" class="btn btn--pink">Go Back</a>
            </div>
        </div>
    </header>

    <main role="main">
        <section>
            <div class="row">
                <p>
                    In 2021, <a target="_blank" rel="noopener noreferrer" href="https://opencv.org/">OpenCV</a> created what would become a wildly successful kickstarter campaign for an experimental line of cameras 
                    with highly versatile functionality.  Enter: <b>OpenCV AI Kit (OAK)</b>.  OAK was the first device of this hardware-software renaissance and was priced at
                    a cool $199.  While the family of OAK devices has <a target="_blank" rel="noopener noreferrer" href="https://shop.luxonis.com/">grown over the years</a>, the core offerings have primarily only seen 
                    performance improvements.  Here are some of the features of this award-winning camera:
                    <ul>
                        <li>4K RGB Capture, at up to 30 FPS</li>
                        <li>1280x800 Stereo Depth Capture, at up to 120 FPS</li>
                        <li>Intel Movidius MyriadX Visual Processing Unit, up to 4 Trillion Operations per second
                            <ul>
                                <li>Support for on-board Neural Network Inference</li>
                                <li>Pre/Post processing for on-board AI Pipelines</li>
                            </ul>
                        </li>
                        <li>IMU (Accelerometer and Gyroscope), extracting quaternion orientation at up to 400 Hz</li>
                    </ul>
                </p>
                <p>
                    In this post, I'll be discussing my personal work with the OAK-D Camera.  I have fairly extensive experience developing software with this camera, however
                    Project OAK-Deep focuses on exploring the vast computer vision and robotics applications which can be powered by minimal hardware outside of the OAK-D camera.
                    This project has infrastructure for the following:
                    <ul>
                        <li>Data Collection (Hardware-Specific Training Dataset Curation)</li>
                        <li>Network Training (Classification, Object Detection, and Instance Segmentation)</li>
                        <li>RGB, Depth, IMU Pipeline-interchangeability (Highly customizable, json-based pipeline construction)</li>
                        <li>Custom Neural Inference Pipelines (Object Detection, OpenPose, Facial Keypoints)</li>
                    </ul>
                    I'll go through each of these components at a high level and give some samples of what a user/developer can accomplish with this mini-library.
                </p>

                <h3>Data Collection</h3>

                <p>
                    In <b>data collection</b> mode, the camera will record and save frames from specified sources.  This feature allows the user/developer the option to
                    upload collected data to an AWS S3 Bucket, if specified.  This feature can save rgb frames and depth frames, but at the moment cannot save IMU data or
                    neural network preditions (might happen in a future release).  OAK-D offers onboard video encooding, but I have yet to explore this feature extensively.
                    I'd like to explore efficient compression methods/filetypes, such as the h5 file extension, to handle faster data saving.  I'd also like to enable saving
                    of IMU data and neural network predictions, which would also require timestamps to be provided for each type of frame (rgb and depth), IMU samples, and network 
                    predictions.  For now though, this dataset collection pipeline is stable and users/developers can find their dataset in the <code>./datasets</code> directory.
                </p>

                <h3>Network Training</h3>
                
                <p>
                    Stepping away from the scope of deploying algorithms and systems onto the OAK-D camera, OAK-Deep also supports training, validation, and testing
                    of neural networks in several different core computer vision tasks.  Currently the following is supported:
                </p>

                <ul>
                    <li>Image Classification
                        <ul>
                            <li>Fully Convolutional Network (FCN) Architecture</li>
                            <li>MobileNet V2 Architecture</li>
                        </ul>
                    </li>
                    <li>Object Detection
                        <ul>
                            <li>YOLOv8 Architecture</li>
                        </ul>
                    </li>
                    <li>Instance Segmentation
                        <ul>
                            <li>YOLOv8 Architecture</li>
                        </ul>
                    </li>
                    <li>Image Generation
                        <ul>
                            <li>Vanilla/Basic GAN Architecture</li>
                            <li>Deep Convolutional GAN Architecture</li>
                        </ul>
                    </li>
                </ul>
                    
                <p>
                    This allows users/developers to choose a dataset or input their own custom dataset, 
                    select a network architecture, and run automatic training and validation.  Users/developers also have authority over the hyperparameters for architecture, 
                    training, and logging.
                </p>

                <h3>On-device Pipeline Customization</h3>

                <p>
                    Other modes exist, such as <b>facial landmark</b> mode and <b>display</b> mode, and the parameters relating to the OAK-D camera are in the oak_config.MODE.json file.
                    Shown below is a demo of the <b>facial landmark</b> mode, with depth map visualization.
                </p>

                <figure> 
                    <video class="blog__img" preload="auto" controls>
                        <source src="https://drive.google.com/uc?export=download&id=14zNifYWMzwMWFfGi1orIlli3ETcLLK7N" type='video/mp4'>
                    </video>
                    <figcaption class="blog__img">
                        Figure 1: Demonstration of <b>facial landmark</b> mode, with depth map visualization.  Notice that the network predictions plotted on the rgb image are distorted
                        This is purely the result of how the frames are visualized.  I use fullscreen OpenCV visualization, which stretches and squashes the frames to fit the display.
                    </figcaption>
                </figure>

                <h3>Off-device Pipeline Customization</h3>
                
                <p>
                    Similarly, there are pipelines which leverage off-device functionality.  The parameters which control these pipelines are also in the oak_config.MODE.json file.
                </p>
            </div>
        </section>
    </main>


  <!-- ***** Footer ***** -->

    <footer role="contentinfo" class="footer">
        <div class="row">
            <!-- Update the links to point to your accounts -->
            <ul class="footer__social-links">
                <li class="footer__social-link-item">
                    <a href="https://twitter.com/nisarhassan12/" title="Link to Twitter Profile">
                        <img src="../../assets/icons/twitter.svg" class="footer__social-image" alt="Twitter">
                    </a>
                </li>
                <li class="footer__social-link-item">
                    <a href="https://github.com/andrew-garrett/" title="Link to Github Profile">
                        <img src="../../assets/icons/github.svg" class="footer__social-image" alt="Github">
                    </a>
                </li>
                <li class="footer__social-link-item">
                    <a href="https://codepen.io/nisar_hassan" title="Link to Codepen Profile">
                        <img src="../../assets/icons/codepen.svg" class="footer__social-image" alt="Codepen">
                    </a>
                </li>
                <li class="footer__social-link-item">
                    <a href="https://www.linkedin.com/in/andrew-garrett-939a94143/">
                        <img src="../../assets/icons/linkedin.svg" title="Link to Linkedin Profile" class="footer__social-image" alt="Linkedin">
                    </a>
                </li>
            </ul>

            <!-- If you give me some credit by keeping the below paragraph, will be huge for me ðŸ˜Š Thanks. -->
            <p>
                &copy; 2020 - Template designed & developed by <a href="https://nisar.dev" class="link">Nisar</a>.
            </p>
            <div class="footer__github-buttons">
                <iframe
                    src="https://ghbtns.com/github-btn.html?user=nisarhassan12&repo=portfolio-template&type=watch&count=true"
                    frameborder="0" scrolling="0" width="170" height="20" title="Watch Portfolio Template on GitHub">
                </iframe>
            </div>
        </div>
    </footer>

    <a href="#top" class="back-to-top" title="Back to Top">
        <img src="../../assets/icons/arrow-up.svg" alt="Back to Top" class="back-to-top__image"/>
    </a>
    <script src="../../index.js"></script>
</body>

</html>