<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="shortcut icon" type="image/png" href="../../assets/images/FAVICON.png" />

    <!-- Put your site title here -->
    <title>
        OAK-Deep
    </title>

    <meta name="description" content="Project OAK-Deep">
    <!-- Add some coding keywords below, Ex: (React, CSS etc) -->
    <meta name="keywords" content="Andrew Garrett, Robotics, Computer Vision, Deep Learning, Machine Learning" />
    <link rel="stylesheet" href="../../index.css" />
</head>

<body>

  <!-- ***** Header ***** -->

    <header class="header" role="banner" id="top">
        <div class="row">
            <nav class="nav" role="navigation">
                <ul class="nav__items">
                  <!-- <li class="nav__item">
                    <a href="../../index.html" class="nav__link">home</a>
                  </li> -->
                </ul>
            </nav>
        </div>
        <div class="header__text-box row">
            <div class="header__text">
                <h1 class="heading-primary">
                    <!-- Project title -->
                    <span>
                        Project OAK-Deep
                        <a   href="https://github.com/andrew-garrett/oakd-research" title="View Source Code" style="float: right;">
                            <img src="../../assets/icons/github.svg" height="50px" alt="GitHub">
                        </a>
                    </span>
                </h1>
                <!-- Put a small paragraph about the project -->
                <p>An experimental toolbox for deep computer vision on the OAK-D Camera</p>
                <a href="../../index.html#projects" class="btn btn--pink">Go Back</a>
            </div>
        </div>
    </header>

    <main role="main">
        <section>
            <div class="row">
                <p>
                    In 2021, <a   href="https://opencv.org/">OpenCV</a> created what would become a wildly successful kickstarter campaign for an experimental line of cameras 
                    with highly versatile functionality.  Enter: <b>OpenCV AI Kit (OAK)</b>.  OAK was the first device of this hardware-software renaissance and was priced at
                    a cool $199.  While the family of OAK devices has <a   href="https://shop.luxonis.com/">grown over the years</a>, the core offerings have primarily only seen 
                    performance improvements.  Here are some of the features of this award-winning camera:
                    <ul>
                        <li>4K RGB Capture, at up to 30 FPS</li>
                        <li>1280x800 Stereo Depth Capture, at up to 120 FPS</li>
                        <li>Intel Movidius MyriadX Visual Processing Unit (VPU), up to 4 Trillion Operations per second
                            <ul>
                                <li>Support for on-board Neural Network Inference</li>
                                <li>Pre/Post processing for on-board AI Pipelines</li>
                            </ul>
                        </li>
                        <li>IMU (Accelerometer, Gyroscope, Magnetometer), orientation estimates at up to 400 Hz</li>
                    </ul>
                </p>
                <p>
                    In this post, I'll be discussing my personal work with the OAK-D Camera.  I have fairly extensive experience developing software with this camera, however
                    Project OAK-Deep focuses on exploring the vast computer vision and robotics applications which can be powered by minimal hardware outside of the OAK-D camera.
                    This project has infrastructure for the following:
                    <ul>
                        <li>Network Training (Classification, Object Detection, Instance Segmentation, Image Generation)</li>
                        <li>Data Collection (Hardware-Specific Training Dataset Curation)</li>
                        <li>RGB, Depth, IMU Pipeline-interchangeability (Highly customizable, json-based pipeline construction)</li>
                        <li>Custom Neural Inference Pipelines (Object Detection, OpenPose, Facial Keypoints)</li>
                    </ul>
                </p>

                <h3>Network Training</h3>
                
                <p>
                    Stepping away from the scope of deploying algorithms and systems onto the OAK-D camera, OAK-Deep also supports training, validation, and testing
                    of neural networks in several different core computer vision tasks.  Currently the following is supported:
                </p>

                <ul>
                    <li>Image Classification
                        <ul>
                            <li>Fully Convolutional Network (FCN) Architecture</li>
                            <li>MobileNet V2 Architecture</li>
                        </ul>
                    </li>
                    <li>Object Detection
                        <ul>
                            <li>YOLOv8 Architecture</li>
                        </ul>
                    </li>
                    <li>Instance Segmentation
                        <ul>
                            <li>YOLOv8 Architecture</li>
                        </ul>
                    </li>
                    <li>Image Generation
                        <ul>
                            <li>Basic GAN Architecture</li>
                            <li>Deep Convolutional GAN (DCGAN) Architecture</li>
                            <li>Deep Variational AutoEncoder (VAE) Architecture</li>
                        </ul>
                    </li>
                </ul>
                    
                <p>
                    With a wide variety of tasks and networks to train such tasks, the user/developer can also choose from a vast number of different datasets.  OAK-Deep
                    currently supports datasets sourced from <a target="_blank" rel="noopener noreferrer" href="https://universe.roboflow.com">Roboflow</a>, <a target="_blank" rel="noopener noreferrer" href="https://pytorch.org/vision/stable/datasets.html">Pytorch</a>,
                    <a target="_blank" rel="noopener noreferrer" href="https://docs.ultralytics.com/datasets/?h=dataset">Ultralytics</a>, and even <b>custom datasets</b> with proper formatting.  This puts
                    hundreds, possibly even <em>thousands</em>, of datasets at the fingertips of developers.
                </p>
                <p>
                    With customization of task, network, and dataset, users/developers can stick to the defaults for training their own model, or they can choose hyperparameters
                    which are best suited for their task.  Developers can control a set of well-studied training hyperparameters, such as batch size and number of epochs, learning rate, momentum, and weight decay,
                    as well as optimizer and loss function.  For more seasoned deep learning developers, these parameters can also be tuned/explored relatively automatically, with 
                    <a target="_blank" rel="noopener noreferrer" href="https://docs.wandb.ai/guides/sweeps#:~:text=There%20are%20two%20components%20to,them%20to%20run%20model%20training.">Weights and Biases Sweeps</a>.  Here are some 
                    examples of projects I have created with OAK-Deep, visualized with <a target="_blank" rel="noopener noreferrer" href="https://wandb.ai/site">Weights and Biases</a>.
                </p>

                <ul>
                    <li><a target="_blank" rel="noopener noreferrer" href="https://wandb.ai/andrew-garrett/oakd-research-generation/sweeps/83ck3gdj?workspace=user-">(a bit nightmarish) Human Face Generation Sweep</a></li>
                    <li><a target="_blank" rel="noopener noreferrer" href="https://wandb.ai/andrew-garrett/oakd-research-segmentation?workspace=user-andrew-garrett">Amazon Armbench Object Segmentation</a></li>
                    <li><a target="_blank" rel="noopener noreferrer" href="https://wandb.ai/andrew-garrett/oakd-research-detection?workspace=user-andrew-garrett">Weed Object Detection (don't worry, not the good kind of weed)</a></li>
                </ul>

                <p>
                    I have designed this subset of the project to allows users to generally train and test their own neural networks, but my next steps are to fit it as a component in the OAK-Deep
                    ecosystem by offering automated deployment of such models onto the OAK-D camera.
                </p>

                <h3>Data Collection</h3>

                <p>
                    In <b>data collection</b> mode, the camera will record and save frames from specified sources.  This feature allows the user/developer the option to
                    upload collected data to an AWS S3 Bucket, if specified.  This feature can save rgb frames and depth frames, but at the moment cannot save IMU data or
                    neural network preditions (might happen in a future release).  OAK-D offers onboard video encooding, but I have yet to explore this feature extensively.
                    I'd like to explore efficient compression methods/filetypes, such as the h5 file extension, to handle faster data saving.  I'd also like to enable saving
                    of IMU data and neural network predictions, which would also require timestamps to be provided for each type of frame (rgb and depth), IMU samples, and network 
                    predictions.  For now though, this dataset collection pipeline is stable and users/developers can find their dataset in the <code>./datasets</code> directory.
                </p>


                <h3>On-device Pipeline Customization</h3>

                <p>
                    Other modes exist, such as <b>facial landmark</b> mode and <b>display</b> mode, and the parameters relating to the OAK-D camera are in the oak_config.MODE.json file.
                    Shown below is a demo of the <b>facial landmark</b> mode, with depth map visualization.
                </p>

                <figure> 
                    <iframe class="blog__img" src="https://drive.google.com/file/d/14zNifYWMzwMWFfGi1orIlli3ETcLLK7N/preview" width="1920" height="1080"></iframe>
                    <figcaption class="blog__img">
                        Figure 1: Demonstration of <b>facial landmark</b> mode, with depth map visualization.  Notice that the network predictions plotted on the rgb image are distorted
                        This is purely the result of how the frames are visualized.  I use fullscreen OpenCV visualization, which stretches and squashes the frames to fit the display.
                    </figcaption>
                </figure>

                <h3>Off-device Pipeline Customization</h3>
                
                <p>
                    Similarly, there are pipelines which leverage off-device functionality.  The parameters which control these pipelines are also in the oak_config.MODE.json file.  Currently,
                    most of th off-device pipeline functionality is geared toward visualizing network outputs.  My next steps will be to implement a basic SLAM pipeline with visualization, as well
                    as AprilTag/ARuCo Marker AR visualization.
                </p>
            </div>
        </section>
    </main>


  <!-- ***** Footer ***** -->

    <footer role="contentinfo" class="footer">
        <div class="row">
            <!-- Update the links to point to your accounts -->
            <ul class="footer__social-links">
                <li class="footer__social-link-item">
                    <a target="_blank" rel="noopener noreferrer" href="https://www.instagram.com/_.a.n.d.r.3.w._/" title="Link to Instagram Profile">
                        <img src="../../assets/icons/twitter.svg" class="footer__social-image" alt="Instagram">
                    </a>
                </li>
                <li class="footer__social-link-item">
                    <a target="_blank" rel="noopener noreferrer" href="https://github.com/andrew-garrett/" title="Link to Github Profile">
                        <img src="../../assets/icons/github.svg" class="footer__social-image" alt="Github">
                    </a>
                </li>
                <li class="footer__social-link-item">
                    <a target="_blank" rel="noopener noreferrer" href="https://wandb.ai/andrew-garrett" title="Link to Weights and Biases Profile">
                        <img src="../../assets/icons/wandb.svg" class="footer__social-image" alt="Weights and Biases">
                    </a>
                </li>
                <li class="footer__social-link-item">
                    <a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/andrew-garrett-939a94143/">
                        <img src="../../assets/icons/linkedin.svg" title="Link to Linkedin Profile" class="footer__social-image" alt="Linkedin">
                    </a>
                </li>
            </ul>
        </div>
    </footer>

    <a href="#top" class="back-to-top" title="Back to Top">
        <img src="../../assets/icons/arrow-up.svg" alt="Back to Top" class="back-to-top__image"/>
    </a>
    <script src="../../index.js"></script>
</body>

</html>